{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.2** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-machine-learning/resources/bANLa) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Evaluation\n",
    "\n",
    "In this assignment you will train several models and evaluate how effectively they predict instances of fraud using data based on [this dataset from Kaggle](https://www.kaggle.com/dalpozz/creditcardfraud).\n",
    " \n",
    "Each row in `fraud_data.csv` corresponds to a credit card transaction. Features include confidential variables `V1` through `V28` as well as `Amount` which is the amount of the transaction. \n",
    " \n",
    "The target is stored in the `class` column, where a value of 1 corresponds to an instance of fraud and 0 corresponds to an instance of not fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Import the data from `fraud_data.csv`. What percentage of the observations in the dataset are instances of fraud?\n",
    "\n",
    "*This function should return a float between 0 and 1.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016410823768035772"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_one():\n",
    "    \n",
    "    # Your code here\n",
    "    df = pd.read_csv('fraud_data.csv')\n",
    "    no_of_fraud = len(df[df['Class']>0])\n",
    "    return no_of_fraud/len(df)# Return your answer\n",
    "answer_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0  1.176563  0.323798  0.536927  1.047002 -0.368652 -0.728586  0.084678   \n",
      "1  0.681109 -3.934776 -3.801827 -1.147468 -0.735540 -0.501097  1.038865   \n",
      "2  1.140729  0.453484  0.247010  2.383132  0.343287  0.432804  0.093380   \n",
      "3 -1.107073 -3.298902 -0.184092 -1.795744  2.137564 -1.684992 -2.015606   \n",
      "4 -0.314818  0.866839 -0.124577 -0.627638  2.651762  3.428128  0.194637   \n",
      "\n",
      "         V8        V9       V10   ...         V20       V21       V22  \\\n",
      "0 -0.069246 -0.266389  0.155315   ...   -0.137258 -0.109627 -0.341365   \n",
      "1 -0.626979 -2.274423  1.527782   ...    1.341809  0.652202  0.272684   \n",
      "2  0.173310 -0.808999  0.775436   ...   -0.232185 -0.003802  0.058556   \n",
      "3 -0.007181 -0.165760  0.869659   ...    0.348269  0.130648  0.329445   \n",
      "4  0.670674 -0.442658  0.133499   ...    0.402329 -0.312774 -0.799494   \n",
      "\n",
      "        V23       V24       V25       V26       V27       V28  Amount  \n",
      "0  0.057845  0.499180  0.415211 -0.581949  0.015472  0.018065    4.67  \n",
      "1 -0.982151  0.165900  0.360251  0.195321 -0.256273  0.056501  912.00  \n",
      "2 -0.121177 -0.304215  0.645893  0.122600 -0.012115 -0.005945    1.00  \n",
      "3  0.927656 -0.049560 -1.892866 -0.575431  0.266573  0.414184   62.10  \n",
      "4 -0.064488  0.953062 -0.429550  0.158225  0.076943 -0.015051    2.67  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "5        0\n",
      "6        0\n",
      "7        0\n",
      "8        0\n",
      "9        0\n",
      "10       0\n",
      "11       0\n",
      "12       0\n",
      "13       0\n",
      "14       0\n",
      "15       1\n",
      "16       0\n",
      "17       0\n",
      "18       0\n",
      "19       0\n",
      "20       0\n",
      "21       0\n",
      "22       0\n",
      "23       0\n",
      "24       0\n",
      "25       0\n",
      "26       0\n",
      "27       0\n",
      "28       0\n",
      "29       0\n",
      "        ..\n",
      "21663    0\n",
      "21664    0\n",
      "21665    0\n",
      "21666    0\n",
      "21667    0\n",
      "21668    0\n",
      "21669    0\n",
      "21670    0\n",
      "21671    0\n",
      "21672    0\n",
      "21673    0\n",
      "21674    0\n",
      "21675    0\n",
      "21676    0\n",
      "21677    0\n",
      "21678    0\n",
      "21679    0\n",
      "21680    0\n",
      "21681    0\n",
      "21682    0\n",
      "21683    0\n",
      "21684    0\n",
      "21685    0\n",
      "21686    0\n",
      "21687    0\n",
      "21688    0\n",
      "21689    0\n",
      "21690    0\n",
      "21691    0\n",
      "21692    0\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Use X_train, X_test, y_train, y_test for all of the following questions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('fraud_data.csv')\n",
    "\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Using `X_train`, `X_test`, `y_train`, and `y_test` (as defined above), train a dummy classifier that classifies everything as the majority class of the training data. What is the accuracy of this classifier? What is the recall?\n",
    "\n",
    "*This function should a return a tuple with two floats, i.e. `(accuracy score, recall score)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.96957964601769908, 0.012500000000000001)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_two():\n",
    "    from sklearn.dummy import DummyClassifier\n",
    "    from sklearn.metrics import recall_score\n",
    "    \n",
    "    # Your code here\n",
    "    dummy = DummyClassifier()\n",
    "    dummy.fit(X_train, y_train)\n",
    "    y_pred = dummy.predict(X_test)\n",
    "    score = dummy.score(X_test,y_test)\n",
    "    recall_score = recall_score(y_test,y_pred)\n",
    "    return (score,recall_score)# Return your answer\n",
    "\n",
    "answer_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Using X_train, X_test, y_train, y_test (as defined above), train a SVC classifer using the default parameters. What is the accuracy, recall, and precision of this classifier?\n",
    "\n",
    "*This function should a return a tuple with three floats, i.e. `(accuracy score, recall score, precision score)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.99078171091445433, 0.375, 1.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_three():\n",
    "    from sklearn.metrics import recall_score, precision_score\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    # Your code here\n",
    "    clf = SVC().fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    accuracy_score = clf.score(X_test,y_test)\n",
    "    recall_score = recall_score(y_test,y_pred)\n",
    "    precision_score = precision_score(y_test,y_pred)\n",
    "    \n",
    "    return (accuracy_score,recall_score,precision_score)# Return your answer\n",
    "answer_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Using the SVC classifier with parameters `{'C': 1e9, 'gamma': 1e-07}`, what is the confusion matrix when using a threshold of -220 on the decision function. Use X_test and y_test.\n",
    "\n",
    "*This function should return a confusion matrix, a 2x2 numpy array with 4 integers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5320,   24],\n",
       "       [  14,   66]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_four():\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    # Your code here\n",
    "    clf = SVC(C=1e9,gamma=1e-07).fit(X_train,y_train)\n",
    "    y_predict = clf.decision_function(X_test) > -220\n",
    "    #y_predict = clf.predict(X_test)\n",
    "    confusion = confusion_matrix(y_test, y_predict)\n",
    "\n",
    "    return confusion# Return your answer\n",
    "answer_four()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Train a logisitic regression classifier with default parameters using X_train and y_train.\n",
    "\n",
    "For the logisitic regression classifier, create a precision recall curve and a roc curve using y_test and the probability estimates for X_test (probability it is fraud).\n",
    "\n",
    "Looking at the precision recall curve, what is the recall when the precision is `0.75`?\n",
    "\n",
    "Looking at the roc curve, what is the true positive rate when the false positive rate is `0.16`?\n",
    "\n",
    "*This function should return a tuple with two floats, i.e. `(recall, true positive rate)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.82499999999999996, 0.9375)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_five():\n",
    "        \n",
    "    # Your code here\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    from sklearn.metrics import roc_curve\n",
    "    \n",
    "    lr = LogisticRegression().fit(X_train, y_train)\n",
    "    y_scores_lr = lr.fit(X_train, y_train).decision_function(X_test)\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_scores_lr)\n",
    "    recall_lr = recall[np.where(np.absolute(precision-0.75)<0.001)][0]\n",
    "    \n",
    "    #import matplotlib.pyplot as plt\n",
    "    #%matplotlib notebook\n",
    "    #plt.figure()\n",
    "    #plt.plot(precision, recall, label='Precision-Recall Curve')\n",
    "    #plt.show()\n",
    "    \n",
    "    fpr_lr, tpr_lr, _ = roc_curve(y_test, y_scores_lr)\n",
    "    true_positive_rate_lr = tpr_lr[np.where(np.absolute(fpr_lr-0.16)<0.001)][0]\n",
    "\n",
    "    #plt.figure()\n",
    "    #plt.plot(fpr_lr[:30], tpr_lr[:30])\n",
    "    #plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    #plt.ylabel('True Positive Rate', fontsize=16)\n",
    "\n",
    "    return (recall_lr, true_positive_rate_lr)# Return your answer\n",
    "\n",
    "answer_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Perform a grid search over the parameters listed below for a Logisitic Regression classifier, using recall for scoring and the default 3-fold cross validation.\n",
    "\n",
    "`'penalty': ['l1', 'l2']`\n",
    "\n",
    "`'C':[0.01, 0.1, 1, 10, 100]`\n",
    "\n",
    "From `.cv_results_`, create an array of the mean test scores of each parameter combination. i.e.\n",
    "\n",
    "|      \t| `l1` \t| `l2` \t|\n",
    "|:----:\t|----\t|----\t|\n",
    "| **`0.01`** \t|    ?\t|   ? \t|\n",
    "| **`0.1`**  \t|    ?\t|   ? \t|\n",
    "| **`1`**    \t|    ?\t|   ? \t|\n",
    "| **`10`**   \t|    ?\t|   ? \t|\n",
    "| **`100`**   \t|    ?\t|   ? \t|\n",
    "\n",
    "<br>\n",
    "\n",
    "*This function should return a 5 by 2 numpy array with 10 floats.* \n",
    "\n",
    "*Note: do not return a DataFrame, just the values denoted by '?' above in a numpy array. You might need to reshape your raw result to meet the format we are looking for.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.66666667,  0.76086957],\n",
       "       [ 0.80072464,  0.80434783],\n",
       "       [ 0.8115942 ,  0.8115942 ],\n",
       "       [ 0.80797101,  0.8115942 ],\n",
       "       [ 0.80797101,  0.80797101]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_six():    \n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    # Your code here\n",
    "    lr = LogisticRegression()\n",
    "    grid_values = {'penalty': ['l1','l2'],'C':[0.01, 0.1, 1, 10, 100]}\n",
    "    grid_lr = GridSearchCV(lr, param_grid = grid_values,scoring = 'recall')\n",
    "    grid_lr.fit(X_train, y_train) \n",
    "    mean_test_scores = grid_lr.cv_results_.get('mean_test_score').reshape(5,2)\n",
    "    \n",
    "    return mean_test_scores# Return your answer\n",
    "answer_six()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the following function to help visualize results from the grid search\n",
    "def GridSearch_Heatmap(scores):\n",
    "    #%matplotlib notebook\n",
    "    import seaborn as sns\n",
    "    #import matplotlib.pyplot as plt\n",
    "    #plt.figure()\n",
    "    #sns.heatmap(scores.reshape(5,2), xticklabels=['l1','l2'], yticklabels=[0.01, 0.1, 1, 10, 100])\n",
    "    #plt.yticks(rotation=0);\n",
    "\n",
    "#GridSearch_Heatmap(answer_six())"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-machine-learning",
   "graded_item_id": "5yX9Z",
   "launcher_item_id": "eqnV3",
   "part_id": "Msnj0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
